{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7499b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "from langchain import hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43446793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load Environment Variables ---\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a236becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "pdf_path = \"attention.pdf\" \n",
    "mongo_uri = os.getenv(\"MONGO_URI\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\") # For LangSmith tracing\n",
    "langchain_tracing_v2 = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "langchain_project = os.getenv(\"LANGCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e744134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables for LangChain/Google SDKs\n",
    "os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = langchain_tracing_v2\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = langchain_project\n",
    "\n",
    "db_name = \"RegPractise\"\n",
    "collection_name = \"attention\"\n",
    "vector_index_name = \"vector_index\" # Your MongoDB Atlas vector index name\n",
    "\n",
    "if not all([mongo_uri, google_api_key]):\n",
    "    raise ValueError(\"Please ensure MONGO_URI and GOOGLE_API_KEY are set in your .env file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35403ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Data Ingestion ---\n",
      "Loaded 11 pages from PDF\n",
      "Ingesting documents into MongoDB Atlas Vector Search...\n",
      "All documents embedded and stored in MongoDB Atlas via LangChain!\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Initial Setup and Data Ingestion (LangChain Way) ---\n",
    "print(\"\\n--- Starting Data Ingestion ---\")\n",
    "\n",
    "\n",
    "# Load Documents\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} pages from PDF\")\n",
    "\n",
    "\n",
    "# Initialize Embeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "\n",
    "# Connect to MongoDB Atlas\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[db_name]\n",
    "collection = db[collection_name]\n",
    "\n",
    "\n",
    "# Optional: Clear existing documents if you want to re-ingest every time\n",
    "collection.delete_many({})\n",
    "\n",
    "\n",
    "# Ingest Documents into MongoDB Atlas Vector Search\n",
    "# This will check if documents already exist based on content and metadata (if applicable)\n",
    "# or just insert if the collection is empty.\n",
    "print(\"Ingesting documents into MongoDB Atlas Vector Search...\")\n",
    "vector_store = MongoDBAtlasVectorSearch.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    collection=collection,\n",
    "    index_name=vector_index_name\n",
    ")\n",
    "print(\"All documents embedded and stored in MongoDB Atlas via LangChain!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9632b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building the RAG Pipeline ---\n",
      "Retriever configured.\n",
      "[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n",
      "RAG chain built successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Retrieval Augmented Generation (RAG) Pipeline ---\n",
    "print(\"\\n--- Building the RAG Pipeline ---\")\n",
    "\n",
    "\n",
    "# Define the Retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5}) # Get top 5 relevant chunks\n",
    "print(\"Retriever configured.\")\n",
    "\n",
    "\n",
    "# Initialize the Large Language Model (LLM)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", temperature=0.3)\n",
    "\n",
    "\n",
    "# Define the Prompt Template - FROM LANGCHAIN HUB\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "import pprint\n",
    "pprint.pprint(prompt.messages)\n",
    "\n",
    "\n",
    "# Build the RAG Chain using LangChain Expression Language (LCEL)\n",
    "def format_docs(docs):\n",
    "    \"\"\"Helper function to concatenate the page_content of retrieved documents.\"\"\"\n",
    "    # Ensure it works whether docs are (Document, score) tuples or just Document objects\n",
    "    if docs and isinstance(docs[0], tuple) and isinstance(docs[0][0], Any):\n",
    "         return \"\\n\\n\".join(doc[0].page_content for doc in docs)\n",
    "    else:\n",
    "         return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | RunnableLambda(format_docs), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"RAG chain built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25405350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing the RAG Pipeline ---\n",
      "\n",
      "Question 1: What is the main innovation proposed in the paper?\n",
      "Answer 1: The paper proposes the Transformer, a novel network architecture based solely on attention mechanisms.  It replaces recurrent and convolutional layers with multi-headed self-attention, enabling greater parallelization and faster training. This results in superior translation quality, as demonstrated by state-of-the-art results on WMT 2014 English-to-German and English-to-French translation tasks.\n",
      "\n",
      "Question 2: How do they handle position information without recurrence?\n",
      "Answer 2: Positional encodings are added to the input embeddings at the bottom of the encoder and decoder stacks.  These encodings use sine and cosine functions of different frequencies and have the same dimension as the embeddings. This allows the model to utilize sequence order, despite lacking recurrence or convolution.\n",
      "\n",
      "Question 3: What is the capital of France?\n",
      "Answer 3: This document discusses the \"Transformer\" model for machine translation and does not contain information about the capital of France.  Therefore, I don't know the answer to your question.\n"
     ]
    }
   ],
   "source": [
    "# --- Testing Your RAG Pipeline ---\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Testing the RAG Pipeline ---\")\n",
    "\n",
    "    # Example 1\n",
    "    user_query_1 = \"What is the main innovation proposed in the paper?\"\n",
    "    print(f\"\\nQuestion 1: {user_query_1}\")\n",
    "    answer_1 = rag_chain.invoke(user_query_1)\n",
    "    print(f\"Answer 1: {answer_1}\")\n",
    "\n",
    "    # Example 2\n",
    "    user_query_2 = \"How do they handle position information without recurrence?\"\n",
    "    print(f\"\\nQuestion 2: {user_query_2}\")\n",
    "    answer_2 = rag_chain.invoke(user_query_2)\n",
    "    print(f\"Answer 2: {answer_2}\")\n",
    "\n",
    "    # Example 3 (Outside of context)\n",
    "    user_query_3 = \"What is the capital of France?\"\n",
    "    print(f\"\\nQuestion 3: {user_query_3}\")\n",
    "    answer_3 = rag_chain.invoke(user_query_3)\n",
    "    print(f\"Answer 3: {answer_3}\") # Should indicate it can't find the answer in the context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5328d770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
